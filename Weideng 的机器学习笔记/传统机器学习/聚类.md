# 聚类

## K 均值聚类算法（K-means Clustering Algorithm, K-means）

K-means 是一种启发式迭代求解的聚类分析算法，它是最著名的划分聚类算法，由于简洁和效率使得它成为所有聚类算法中最广泛使用的。

给定一个数据点集合和需要的聚类数目k，k由用户指定，k均值算法根据某个距离函数反复把数据分入k个聚类中。

### 算法流程

1. 随机选取 $K$ 个聚类中心（以下简称“中心”）。
2. 计算每个对象到各中心的距离，将对方分配到与其距离最小的中心，即代表“聚为一类”。
3. 在每个中心代表的类中再选取新的中心。
4. 重复对各中心分配对象，直到满足终止条件。

终止条件：

- 没有（或最小数目）对象被重新分配给不同的聚类。
- 没有（或最小数目）聚类中心再发生变化。
- 误差平方和局部最小。

示意图：

![image-20201107101122119](images/聚类/image-20201107101122119.png)

### 优劣

优点：

- 原理较简单，实现较容易，收敛速度快。 

- 聚类效果较优。 

- 算法的可解释度比较强。 

- 主要需要调参的参数仅仅是簇数 $k$。

缺点：

- $k$ 值的选取不好把握。
- 对于不是凸的数据集比较难收敛。
- 如果各隐含类别的数据不平衡，比如各隐含类别的数据量严重失衡，或者各隐含类别的方差不同，则聚类效果不佳。 
- 最终结果和初始点的选择有关，容易陷入局部最优。
- 对噪音和异常点比较敏感。

### 初始化优化 K-means++

 $k$ 个初始化的中心的选择对聚类结果和运行时间都有很大的影响，因此需要选择合适的 $k$ 个中心。如果仅仅是完全随机的选择，有可能导致算法收敛很慢。

优化策略：

1. 随机选择第一个中心。
2. 对于数据集中每一个点，计算其与已选择的各中心中，与其最近的中心的距离 $D(x)$。
3. 选取 $D(x)$ 最大者作为新的中心。
4. 重复计算，直到选择出 $k$ 个中心。
5. 执行标准 K-means 算法。

### 距离计算优化 elkan K-means

在传统的 K-Means 算法中，在每轮迭代时要计算所有样本点到所有质心的距离，计算量巨大。

elkan K-Means 利用了**两边之和大于等于第三边**，以及**两边之差小于第三边**的三角形性质，来减少距离的计算。

优化策略：

对于 $1$ 个样本点 $x$ 和 $2$ 个中心 $\mu_1$，$\mu_2$，首先计算出两个中心间的距离 $D(\mu_1, \mu_2)$。

前者：

后续计算中，如果发现 $2D(x, \mu_1) \le D(\mu_1, \mu_2)$，则必有 $D(x, \mu_1) \le D(x, \mu_2)$，这样就不必再计算 $D(x, \mu_2)$。

后者：

一定有 $D(x, \mu_2) \ge max\{0,\ D(x, \mu_1) - D(\mu_1, \mu_2)\}$。

利用以上两个规律，elkan K-Means 比起传统的 K-Means 迭代速度有很大的提高。但是如果我们的样本的特征是稀疏的，有缺失值的话，这个方法就不适用了，此时某些距离无法计算，则不能使用该算法。

前者简单有效，后者对简化运算量意义不大。

### 大样本优化 Mini Batch K-Means

顾名思义，Mini Batch，也就是用样本集中的一部分的样本来做传统的 K-Means，这样可以避免样本量太大时的计算难题，算法收敛速度大大加快。当然此时的代价就是我们的聚类的精确度也会有一些降低。一般来说这个降低的幅度在可以接受的范围之内。

在 Mini Batch K-Means 中，我们会选择一个合适的批样本大小 batch size，我们仅仅用 batch size 对应的样本个数来做 K-Means 聚类。

这些 batch size 的样本一般通过**无放回的随机采样**得到。

优化策略：

1. 从数据集中随机抽取一些数据形成小批量，把他们分配给最近的中心。

2. 更新中心。

与 K-means 相比，数据的更新是在每一个小的样本集上。对于每一个小批量，通过计算平均值得到更新中心，并把小批量里的数据分配给该中心，随着迭代次数的增加，这些中心的变化是逐渐减小的，直到中心稳定或者达到指定的迭代次数，停止计算。

## DBSCAN（Density-Based Spatial Clustering of Applications with Noise）

DBSCAN 是一个比较有代表性的基于[密度](https://baike.baidu.com/item/密度/718381)的[聚类算法](https://baike.baidu.com/item/聚类算法/1252197)。与划分和层次聚类方法不同，它将簇定义为密度相连的点的最大集合，能够把具有足够高密度的区域划分为簇，并可在噪声的空间数据库中发现任意形状的聚类。

### 基本概念

DBSCAN算法将数据点分为三类：
　　1.核心点：在半径Eps内含有超过MinPts数目的点。
　　2.边界点：在半径Eps内点的数量小于MinPts,但是落在核心点的邻域内的点。
　　3.噪声点：既不是核心点也不是边界点的点。

示例：

![image-20201213102002894](images/聚类/image-20201213102002894.png)

上图中红色为核心点，黄色为边界点，蓝色为噪声点。

![image-20201213101631026](images/聚类/image-20201213101631026.png)

### 算法流程

DBScan需要两个参数： 扫描半径 (eps)和最小包含点数(minPts)。 任选一个未被访问(unvisited)的点开始，找出与其距离在eps之内(包括eps)的所有附近点。

如果 附近点的数量 ≥ minPts，则当前点与其附近点形成一个簇，并且出发点被标记为已访问(visited)。 然后递归，以相同的方法处理该簇内所有未被标记为已访问(visited)的点，从而对簇进行扩展。

如果 附近点的数量 < minPts，则该点暂时被标记作为噪声点。

如果簇充分地被扩展，即簇内的所有点被标记为已访问，然后用同样的算法去处理未被访问的点。

### 伪代码

DBSCAN的核心思想是从某个核心点出发，不断向密度可达的区域扩张，从而得到一个包含核心点和边界点的最大化区域，区域中任意两点密度相连。

```python
DBSCAN(D, eps, MinPts)
   C = 0                                          
   for each unvisited point P in dataset D        
      mark P as visited                           
      NeighborPts = regionQuery(P, eps)      //计算这个点的邻域    
      if sizeof(NeighborPts) < MinPts             
         mark P as NOISE                          
      else                                        
         C = next cluster                   //作为核心点，根据该点创建一个新类簇
         expandCluster(P, NeighborPts, C, eps, MinPts)   //根据该核心点扩展类别
          
expandCluster(P, NeighborPts, C, eps, MinPts)
   add P to cluster C                            //扩展类别，核心点先加入
   for each point P' in NeighborPts                    
      if P' is not visited
         mark P' as visited                              
         NeighborPts' = regionQuery(P', eps)    //如果该点为核心点，则扩充该类别
         if sizeof(NeighborPts') >= MinPts
            NeighborPts = NeighborPts joined with NeighborPts'
      if P' is not yet member of any cluster   //如果邻域内点不是核心点，并且无类别，比如噪音数据，则加入此类别
         add P' to cluster C
          
regionQuery(P, eps)                                       //计算点P的邻域
   return all points within P's eps-neighborhood
```

![image-20201213103928380](images/聚类/image-20201213103928380.png)

### 优缺点

优点：

- 与K-means方法相比，DBSCAN不需要事先知道要形成的簇类的数量。

- 与K-means方法相比，DBSCAN可以发现任意形状的簇类。

- 同时，DBSCAN能够识别出噪声点。

- DBSCAN对于数据库中样本的顺序不敏感，即Pattern的输入顺序对结果的影响不大。但是，对于处于簇类之间边界样本，可能会根据哪个簇类优先被探测到而其归属有所摆动。

缺点：

- DBScan不能很好反映高维数据。

- DBScan不能很好反映数据集以变化的密度。

- 如果样本集的密度不均匀、聚类间距差相差很大时，聚类质量较差。

### 对比

![image-20201213104647281](images/聚类/image-20201213104647281.png)

![image-20201213104657789](images/聚类/image-20201213104657789.png)

![image-20201213104708647](images/聚类/image-20201213104708647.png)