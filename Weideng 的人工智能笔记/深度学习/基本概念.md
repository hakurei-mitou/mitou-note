# 基本概念

- 神经网络（Neural Network，NN）

	- 神经元（Neuron）

	- 层（Layer）

		神经网络的层数一般指隐藏层的数目。

- 全连接神经网络（Fully Connected Neural Network）

	有时也称为深度神经网络（Deep Neural Network，DNN）。

- 前馈神经网络（Feedforward Neural Network，FNN）

- 卷积神经网络（Convolutional Neural Network，CNN）

- 循环神经网络（Recurrent Neural Network，RNN）

## 基本结构（全连接神经网络）

### 神经元

- 输入参数 $x_j$
- 各连接参数权重 $w_j$
- 偏置项 $b$ （Bias）
- 激活函数 $A$
- 输出参数 $a$

$$
a = A(b + \sum\limits_j w_jx_j)
$$

将激活函数 $A()$ 的输入称为 $r$ ，一般取 $r$ 为多元线性函数，因为线性函数能够同时作用多个参数的最简单的函数，应当尽量保持神经元的结构简单，把复杂度交由网络实现。

### 单层神经网络

由多个神经元加和组成。

- 输入层

	- 输入参数 $x_j$
- 隐藏层

	- 第 $i$ 个神经元
	- 神经元输入参数 $x_{ij}$
	- 连接权重 $w_{ij}$
	- 神经元偏置项 $b_i$
	- 激活函数 $A_i$
	- 神经元输出参数 $a_i$
- 输出层

	- 放大系数 $c_i$
	- 网络偏置项 $b_n$
	- 输出参数 $y_n$

$$
\begin{cases}
x_{ij} = x_j \\\\
a_i = A(b_i + \sum\limits_j w_{ij} x_{ij}) \\\\
y_n = b_n + \sum\limits_i c_ia_i
\end{cases}
$$

![image-20220603105325177](images/基本概念/image-20220603105325177.png)

即：

![image-20220603140617468](images/基本概念/image-20220603140617468.png)

### 多层神经网络

- 输入层

	- 第一个隐藏层的输入参数 $x_j$
- 第 $l$ 个隐藏层，共 $m$ 层。

	- 第 $i$ 个神经元
	- 神经元输入参数 $x^{(l)}_{ij}$
	- 连接权重 $w^{(l)}_{ij}$
	- 神经元偏置项 $b^{(l)}_i$
	- 激活函数 $A^{(l)}_i$
	- 神经元输出参数 $a^{(l)}_i$
- 输出层

	- 最后一个隐藏层的输出参数 $a^{(m)}_i$
	- 放大系数 $c_i$ （相当于连接权重）
	- 网络偏置项 $b_n$
	- 输出参数 $y_n$

$$
\begin{cases}
x^{(1)}_{ij} = x_j \\
\vdots\\
\begin{cases}
x^{(l)}_{ij} = a^{(l - 1)}_j \\\\
a^{(l)}_i = A^{(l)}_i(b^{(l)}_i + \sum\limits_j w^{(l)}_{ij}x^{(l)}_{ij})
\end{cases} \\
\vdots \\
y_n = b_n + \sum\limits_i c_ia^{(m)}_i
\end{cases}
$$

![image-20220603140454580](images/基本概念/image-20220603140454580.png)

**整个神经网络就是一个多层的嵌套函数，越接近输出参数，就处于越外层。**

## 神经网络梯度

在神经网络中一个样本的训练过程，一个 batch 的梯度一般取样本梯度的平均值：

- 前向传播（Forward Propagation / Forward Pass）

  神经网络中整个从输入参数得到输出参数的计算过程。

  - 神经网络的整体前向传播的计算过程是嵌套函数中**由内而外**的。
  - 越接近输出参数，就越外层。

- 反向传播（Backpropagation / Backward Pass，BP 算法）

  使用梯度下降更新参数的过程。

  - 损失函数的参数为神经网络的输出参数，因此**损失函数是对神经网络嵌套函数的外加一层嵌套**。
  - 需要使用**链式法则（Chain Rule）**求**由外而内**求**损失函数**对参数的偏导。

  - 神经网络的参数连接形式与链式法则中变量依赖关系形式一致，可以把各层偏导结果存储下来，逐层更新参数。
