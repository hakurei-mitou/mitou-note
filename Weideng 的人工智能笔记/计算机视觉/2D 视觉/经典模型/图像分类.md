## 图像分类

以下模型基于 ImageNet 数据集。

ImageNet 图像分类大赛历年冠军：（准确率超越人类后，新的模式都是针对训练集中的特殊的样本进行优化，缺少迁移能力，该大赛已停止举办）

![image-20230303202422258](images/图像分类/image-20230303202422258.png)

### AlexNet

[ImageNet Classification with Deep Convolutional Neural Networks 2012](http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf)

![image-20230228201556990](images/图像分类/image-20230228201556990.png)

![image-20230228202616855](images/图像分类/image-20230228202616855.png)

- 提出卷积层加全连接层的网络结构。
- 首次使用 RELU 作为激活函数。
- 首次提出使用 Dropout 控制过拟合。
- 使用 SGDM，即加入动量的小批次梯度下降算法加速收敛。
- 使用数据增强抑制过拟合。
- 使用 GPU 进行并行训练

### ZFNet

[Visualizing and Understanding Convolutional Networks 2013](https://arxiv.org/pdf/1311.2901.pdf)

与 AlexNet 结构基本一致，做了一些微小的改动。

![image-20230228204144542](images/图像分类/image-20230228204144542.png)

- 对 CNN 原理进行了一些可视化。
- CNN 浅层学习到的是一些微小的基本构成元素，深层学到的结构关系（高级特征）更重要，所以一般越深层：
	- 使用越多卷积核。（即越多参数，通道，层宽度）
	- 增加串联的卷积层数量。（串联的中间无池化，减少高级特征的丢失）


### VGG

[Very Deep Convolutional Networks for Large-Scale Image Recognition 2014](https://arxiv.org/pdf/1409.1556v6.pdf)

又叫 VGG16，有 16 层，后续有 VGG19，但能力提升不高。

![image-20230228202554243](images/图像分类/image-20230228202554243.png)

- 网络越深，性能越好。

- 串联的小卷积核可以得到和大卷积核相同大小的感受野，而串联的小卷积核非线性能力更强。

- 池化后的卷积层应当将卷积核数量加倍。

- 归一化层对卷积作用较小，而且耗资源，不建议使用。

- 为何卷积核增加到 512 就不再增加了？

	第一个全连接层含 102M 参数（最后一层卷积作为输入），占总参数的 74% 。

### GoogLeNet

[Going deeper with convolutions 2014](https://arxiv.org/pdf/1409.4842.pdf)

注意下图的输入在最下方：

![image-20230228204625879](images/图像分类/image-20230228204625879.png)

- 提出了一种 Inception 的结构，能保留输入信号中的更多特征信息。
- 去掉了 AlexNet 的前两个全连接层，并采用平均池化，使得 GoogLeNet 只有 500 万参数（无需 dropout），比 AlexNet 少了 12 倍。
- 在训练过程中，对网络引入辅助分类器，用以回传梯度，克服了训练中的梯度消失问题。

比 VGG：

- 层数更深。
- 参数更少。
- 计算效率更高。
- 非线性表达能力更强。

#### naive inception

串联结构（如 VGG）存在的问题：后层卷积层只能处理前层输出的特征图，前层可能因某些原因（如感受野大小限制）丢失重要信息，后层无法找回。

inception 模块的并行的 1x1 卷积和 pooling 尽量多的保留了输入数据的信息。

![image-20230301194757248](images/图像分类/image-20230301194757248.png)

将前层的输出分别做 4 种操作：

- 1 x 1 卷积

	相当于对数据的通道按权重做了线性组合求和，主要用于通过卷积核个数减少特征图的通道数。

	- 在数据通道上（比如图片），某个位置使用多个卷积核描述其响应，而往往一个位置上只有少部分卷积核具有较大的响应（一个位置一般只有少量的特征），故使用 1x1 卷积进行一定程度的线性组合可以相当程度上保留原数据的信息。

- 3 x 3 卷积

	提取特征。

- 5 x 5 卷积

	提取较大的特征。

- 3 x 3 max 池化

	- 非最大化抑制，选取通道上每个卷积位置最大的值做为输出，不改变特征图大小。
	- 该池化 $padding=2,stride=1$ ，所以不改变特征图大小。

保持这 4 个操作的输出特征图大小一致（宽高一致，通道数可以不一致），然后拼接（concatenation）起来输入下一层。

#### inception V1

![image-20230301194825350](images/图像分类/image-20230301194825350.png)

naive inception module 计算量大，于是有了 reduction 的优化：添加了一些 1x1 的卷积用以减少通道数。

### ResNet

[Deep Residual Learning for Image Recognition 2016](https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf)

残差网络（Residual Network，ResNet）也叫深度残差网络（Deep Residual Network，DRN）

#### 残差

- 误差（Error）

	观察值与真实值的差。（测量值与真实值间存在测量误差）

- 残差（Residual）

	预测值与真实值的差。

假设想要找一个 $x$ ，使得 $f(x) = b$ ，给定一个 $x$  的估计值 $x_0$ ，残差就是 $b-f(x_0)$ ，误差就是 $x - x_0$ 。

#### 梯度退化

也叫梯度消失，即当神经网络的深度不断增大，训练（优化）越来越难，梯度逐渐衰减，从而准确度不增反降的现象。

#### 恒等映射

假设对一个网络增加**恒等映射（Identity Mapping）**层（输入等于输出的层），模型效果不变。

只要能保持原网络效果，那么将恒等映射换为全连接层，则模型不会变坏；如果更换的层学习到一些知识，则模型会变好。

#### 残差块

残差块一般由两层或三层（可以是全连接层，也可以是卷积层）组成，输入为 $x$ ，两层全连接层的输出为 $F(x)$ ，最终的残差块输出为 $H(x)$ （省略了 bias）：



![image-20220724103854425](images/图像分类/image-20220724103854425.png)

本来的期望输出为 $F(x)$ ，添加恒等连接后的期望输出为 $H(x)$ ，原来的 $F(x) = H(x) - x$  即为残差。

浅层神经网络的输出 $F(x)$ 和深层网络的输出 $H(x)$ ，如果深层网络多的层什么都没学到，那么 $H(x) = F(x)$ ，即性能不会弱于浅层网络，如果深层网络多的部分学到了一些特征，那么整个网络性能就得到提升。

#### 原理

- 正向保留原信息，增强特征

	残差连接将跨越层提取的特征与原信息（浅层信息）叠加，从而达到不丢失原信息（浅层信息）并且增强了特征的功能，类似于将提取到的边缘与原图叠加完成图像锐化的过程。

- 反向抑制梯度消失

	残差连接抑制了依赖链计算中的梯度退化。

$$
\frac {\partial H(\boldsymbol x)} {\partial \boldsymbol x} 
= 
\frac {\partial F(\boldsymbol x)} {\partial \boldsymbol x}
+
\frac {\partial \boldsymbol x} {\partial \boldsymbol  x}
\ \rm as \ A + 1
$$

将残差的相接后的值归属到第 $i$ 层，作为其输出。

其中， $H(\boldsymbol x)$ 为第 $i$ 层的输出 $y_i$ ，$F(\boldsymbol x)$ 为第 $i-1$ 层的输出 $y_{i-1}$ ，$\boldsymbol x$ 为第 $i-1$ 层的输入 $x_{i-1}$，亦是第 $i-2$ 层的输出 $y_{i-2}$ ，有：
$$
\frac {\partial y_i} {\partial y_{i-2}}
=
\frac {\partial y_i} {\partial y_{i-1}}
\frac {\partial y_{i-1}} {\partial y_{i-2}}
+
1\ as\ A + 1
$$
考虑权重的依赖链的计算，对 $j$ 层的权重 $w_j$ ： 
$$
\begin{align}
\frac {\partial L} {\partial w_j}
& =
\frac {\partial y_n} {\partial y_{n-1}}
\frac {\partial y_{n-1}} {\partial y_{n-2}}
\dots
\frac {\partial y_i} {\partial y_{i-1}}
\frac {\partial y_{i-1}} {\partial y_{i-2}}
\dots
\frac {\partial y_{j+1}} {\partial y_j}
\frac {\partial y_j} {\partial w_j} \\
& =
same\_prefix
\dots
\frac {\partial y_i} {\partial y_{i-2}}
\dots
same\_suffix \\
& = 
same\_prefix
\dots
(A + 1)
\dots
same\_suffix
\end{align}
$$
其中，即使 $A = 0$ 或接近 $0$ ，仍然有梯度 $1$ 能保持住到浅层的梯度不会更小，这使得网络的可训练性增强。

注意，$A \lt 0$ 时梯度仍会变小，但其接近 $0$ 时幅度小影响小，其不接近 $0$ 时，只要绝对值比 $1$ 大得多，$1$ 的存在就不会太影响其计算。

- 残差网络也可由模型集成来理解

	在一个模型的基础上，综合其它模型的结果不会使得最终结果变得更坏。当移除掉一个子模型，例如 f2，整个网络的性能不会大幅下降。

![image-20220725193146172](images/图像分类/image-20220725193146172.png)

#### 瓶颈块

堆叠过多的层会导致卷积的运算量巨大，在实际应用中，往往采用 bottleneck 的结构：

![image-20230303195236306](images/图像分类/image-20230303195236306.png)

该结构先使用 1x1 卷积减少通道数，从而减少了下一步 3x3 卷积的运算量，最后再使用 1x1 卷积将通道数增大到原数据的通道数，用以与残差相加。

#### 完整的 ResNet 结构

原论文在构建不同总层数的 ResNet 的设置：

![image-20230303200102532](images/图像分类/image-20230303200102532.png)